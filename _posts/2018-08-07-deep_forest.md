---
title: "Deep Forest 学习笔记"
categories:
- 机器学习
- 算法
tags:
- 学习笔记
toc: true
---

{% include lib/mathjax.html %}

[论文地址](https://github.com/cczzyy13/book/blob/master/Deep_Forest.pdf)

##  一、介绍

传统对深度学习的认知是“深度学习是机器学习中使用深度神经网络（DNN）的一个子领域”，几乎当今所有的深度学习应用都是基于神经网络模型，或者更技术性地说，是可以由反向传播训练的多层的参数化非线性可分模型。虽然深度神经网络很强大，但是也有许多缺点：

1. 超参数太多，表现很依赖于调参，由于参数配置组合接近无穷使得 DNN 训练以及理论分析困难
2. 需要大量数据，在中小规模问题上表现不佳
3. 黑箱模型，决策过程很难理解
4. 神经网络结构需在训练前决定，因此模型复杂度被提前确定，通常大于“所需”复杂度

由此又充分理由探索“非神经网络”(no-NN) 的深度模型。提出以下几个思考：

1. 深度学习能否由非可分模型实现？（深度学习 ？= DNN ）
2. 能否不使用反向传播训练深度模型？（反向传播需要可分性）
3. 能否使用随机森林或者XGBoost来实现深度模型？

文章提出 gcForest (multi-Grained Cascade Forest) 来构建深度森林，一个 no-NN 深度模型。

## 二、灵感

### 2.1 来自 DNNs 的灵感

深度神经网络成功的关键在于表示学习能力(representation learning ability)，而 DNNs 中表示学习的关键在于层层的处理 ( layer-by-layer processing)。

![](https://ws1.sinaimg.cn/large/006tKfTcgy1ftzyitg61rj30w00osdwf.jpg)

随着层由下到上，高级别的抽象特征出现。考虑到单层的神经网络 ( flat networks ) ，无论模型复杂度多大，表现并不理想，我们推测 层层处理是DNNs的重要因素之一。

DNNs 在学习过程中会产生新的特征，而决策树或Boosting Machines等模型使用的是原始特征，没有在学习过程中创造新特征。

DNNs 可以被赋予任意高的模型复杂度而决策树或Boosting Machines等模型不可。

因此，我们推测 DNNs 的背后主要有以下三个关键特点：

1. 层层处理（layer-by-layer processing）
2. 内部特征转化 ( in-model feature transformation )
3. 足够的模型复杂度 ( sufficient model complexity)

###  2.2 来自集成学习的灵感

多个学习器的集成往往比单个学习器有着更好的表现，为了构建一个好的集成，独立的学习器个体需要是精确的以及分化多样的。

有一个 “误差-模糊 分解” 公式如下：

$$ E = \overline{E} - \overline{A} $$

\\(E\\) 表示集成的误差，\\(\overline{E}\\) 表示集成中单个学习器的平均误差，\\(\overline{A}\\) 表示平均的模糊度 (ambiguity) 或者称作多样度 (diversity)。当今并没有一个关于 diversity 的公认的为大众所接受的定义。

在实践中，有一些机制可以用来增强 diversity。

1. data sample manipulation, 例如Bagging中的bootstrap 以及AdaBoost中的抽样。
2. input feature manipulation, 例如生成不同的特征子空间来训练不同的学习器。
3. learning parameter manipulation, 例如神经网络中不同的初始权重以及决策树中不同的划分选择。
4. output representation manipulation, 使用不同的输出表示，例如 ECOC 方法以及 Flipping Output 方法。

## 三、gcForest 方法

###  3.1 Cascade forest structure (联级森林结构)

gcForest 的每一级都是决策树森林的集成，也就是 ensemble of ensembles。模型使用不同类型的森林来增加模型的 diversity。例如每个完全随机森林 (completely-random tree forest) 由500棵完全随机树（在树的每一个节点处随机选择一个特征进行划分，直到每片叶子只包含一类样本）组成，每个随机森林 (random forest) 包含500棵树 (随机的选择\\(\sqrt{d}\\) 个特征，\\(d\\) 为输入特征的数量，然后选择最好的 gini 值进行分裂)。结构如下：

![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu0x18y0suj313u0mun48.jpg)

对每一个样本，每个森林会产生一个类别分布的估计（向量）。对同一森林中的每棵树，计算该样本落到的叶节点中所有训练样本的不同类别的百分比，再计算同一森林中所有树的类别向量的平均来得到类别分布的估计。具体方式如下：

![](https://ws2.sinaimg.cn/large/0069RVTdgy1fu0xagb3clj31300gowjm.jpg)

假如每个类别向量的长度为 \\(L\\)（即\\(L\\)-分类问题），该层有 \\(N\\) 个森林，那么拼接而成的向量长度为 \\(L \times N\\)。在此基础上加上原基础特征向量 \\(x\\) 一起作为下一层的输入。

这里介绍的是最简单的类别向量的构造，只取了样本落入叶节点的类别分布，由于增量特征信息很有限，有可能会被高维的原特征向量的信息压过（淹没），因此可以考虑加入更多的增量特征，例如：

1. 母节点的类别分布
2. 兄弟节点的辅助分布信息

为了防止过拟合，每个森林产生的类别向量使用了 k-折交叉验证。每个样本会被用作 k-1次训练，从而得到 k-1 个类别向量，对其求平均作为下一级的增量特征。在扩张新的层级之后，在验证集上对整个模型进行验证，如果表现增益不明显，训练将被停止。这使得 gcForest 可以自适应决定模型的复杂度从而能够应用到不同规模的训练数据上，而不仅仅是大规模数据。

### 3.2 Multi-Grained Scanning(多粒度扫描)

 DNNs 特别善于处理特征关系，例如 CNN(卷积神经网络)可以很高效的处理像素之间有空间关系的图像数据。RNN 可以高效的处理有时序关系的序列数据。受到启发，gcForest 也使用了多粒度扫描对级联森林进行增强。

多粒度扫描是使用了滑动窗口对原始特征进行扫描，具体过程如下图：

![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu0yf46rw9j30tg0hsq9h.jpg)

### 3.3 gcForest 全过程与超参数

![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu0z3ukf6dj318e0lu7f1.jpg)

假如原始的数据特征是400维，使用100、200、300维的三个不同大小的窗口进行多粒度扫描，这样会分别得到301个100维子样本、201个200维子样本、101个300维子样本。每组样本经过一个完全随机森林和一个随机森林训练后，对于一个3分类问题，100维的采样窗口产生长度为 2 * 301 * 3 = 1806 维的向量，200维的采样窗口产生长度为 2 * 201 * 3 = 1206 维的向量，300维的采样窗口产生长度为 2 * 101 * 3 = 606 维的向量，这三个向量分别用来训练级联森林中第 1 层中的第1、2、3级。训练过程一直到验证集的表现收敛。

下表总结了DNNs 和 gcForest 的超参数：

![](https://ws1.sinaimg.cn/large/0069RVTdgy1fu13it5n05j318s0mk79d.jpg)

可以看到 gcForest 的超参数是：

1. 多粒度扫描的森林数
2. 多粒度扫描中每个森林的决策树数
3. 多粒度扫描中树的停止生长规则
4. 多粒度扫描中滑动窗口的大小与数量
5. 级联中每层的森林数
6. 级联中每个森林的决策树数
7. 级联中树的停止生长规则

## 四、实验

### 4.1 实验设置

- 每一级联中包含4个完全随机森林和4个随机森林，每个森林包含500棵树。
- 类别向量由3-折交叉验证产生，级联的层数由训练自动决定。
- 将训练集的80%作为 growing set，20%作为estimating set。对每一层级，对growing set 使用3-折交叉验证训练该层级中的森林（得到3组不同的森林），求estimating set在三组不同森林上的平均表现，来决定是否停止生长。
- 关于多粒度扫描，\\( d \\) 个原始特征，窗口大小分别取为 \\([d/16], [d/8], [d/4]\\)。

### 4.2 实验结果

实验证明，在图像分类、人脸识别、音乐分类、手势识别、情感分类等问题上都有很好的表现。

### 4.3 低维数据

表现很好

### 4.4 高维数据

比最先进的 DNNs 效果要差，在将深度森林最后一级改为 GBDT 后，效果有所提升。

### 4.5 运行时间

运行时间缩短较多，并且森林的训练可以并行化（有待优化）

### 4.6 多粒度扫描的影响

对于有空间或者时序特征关系的数据，多粒度扫描可以显著的提高表现。

### 4.7 级联结构的影响

有一种设计是将不同粒度扫描产生的向量连接起来作为一个向量进行训练，模型如下：

![](https://ws3.sinaimg.cn/large/0069RVTdgy1fu15wt4qx3j31kw0qkk5f.jpg)

实验表明，\\(gcForest\\) 效果比 \\(gcForest_{conc}\\) 更好。

## 五、未来研究方向

1. 类别向量的形式可以更复杂丰富
2. 算法加速以及减少内存消耗
3. 针对树结构的分布式计算设计











 