---
title: "XGBoost 学习笔记 "
categories:
- 机器学习
tags:
- 算法
- 学习笔记
toc: true
---

{% include lib/mathjax.html %}

## 参考链接

[论文链接](https://github.com/cczzyy13/book/blob/master/XGBoost/XGBoost_A_Scalable_Tree_Boosting_System.pdf)

[陈天奇原文《梯度提升法和Boosted Tree》的修改文](http://www.52cs.org/?p=429)

[Introduction to Boosted Trees, Tianqi Chen](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)

[XGBoost Github主页](https://github.com/dmlc/xgboost)

[XGBoost Documents](http://xgboost.readthedocs.io/en/latest/)

[XGBoost:A Scalable Tree Boosting System](http://www.machinelearning.ru/wiki/images/6/67/MMP_-_xgBoost_-_detailed_description.pdf)

## 1、简介

XGBoost 是一个关于树增强的大规模机器学习系统，是开源的。近年来由于效果极佳被广泛应用于各大机器学习的比赛中，例如 Kaggle、KDDCup 等。XGBoost 如此火爆的一个重要原因是它在各种情境下的可拓展性。它有以下几个创新点：

- introduce a novel sparsity-aware algorithm for parallel tree learning
- propose a theoretically justified weighted quantilesketch
- propose an effective cache-aware block structurefor out-of-core tree learning

## 2、Gradient Tree Boosting

### 2.1 目标函数

对于给定的数据集 \\( D = \\) { \\((x_i, y_i)\\) } \\( (\|D\| = n, x_i \in R^m, y_i \in R)\\)，使用包含 K 个可加函数的树集合来预测输出，模型如下：

![](https://ws2.sinaimg.cn/large/006tNc79gy1ftgcgtef4uj30qq03u3yr.jpg)

其中![](https://ws3.sinaimg.cn/large/006tNc79gy1ftgch7s6q5j30m601qaa9.jpg)

是回归树的集合空间。\\(q\\) 表示每棵树的结构——将样本映射到第几片叶子，\\(T\\) 是树的叶子数量，\\(w\\) 是树的叶子的分数（权重）向量。树结构\\(q\\) 以及树叶权重\\(w\\) 决定了每一个\\(f_k\\)，通过样本在每棵树中分到的分数加起来得到对样本的预测。示意图如下：

![](https://ws2.sinaimg.cn/large/006tNc79gy1ftgcus95r0j30uu0i2grg.jpg)

为了学习模型中的函数集（树集），要最小化以下目标函数：

![](https://ws3.sinaimg.cn/large/006tNc79gy1ftgd080vhgj30nw05et99.jpg)

其中\\(l\\)是可分的凸损失函数，用来衡量预测值与实际lable的差异。\\(\Omega\\) 用于衡量模型的复杂度（回归树叶子的数量以及叶子权重向量的二范数）。

### 2.2 gradient tree boosting

上面得到的极小化函数中包含函数作为参数，无法使用传统的欧几里得空间中的优化算法进行求解。这个模型是使用叠加的模式进行训练的。假设 \\(\hat{y}_i^{(t)}\\) 表示第 \\(t\\) 次迭代中对第 \\(i\\) 个实例的预测，那么我们在第 \\(t\\) 步需要添加 \\(f_t\\) 来使得下式目标函数最小：

![](https://ws1.sinaimg.cn/large/006tNc79gy1ftgfva13qgj30mm03q0t0.jpg)

对上式中的第一项进行泰勒展开到二阶得到：

![](https://ws2.sinaimg.cn/large/006tNc79gy1ftgfyt0ehrj30ts060q3t.jpg)

移除常数项后得到

![](https://ws2.sinaimg.cn/large/006tNc79gy1ftgfzlu8ltj30m80400t2.jpg)

然后将样本按照叶子进行分类，定义 \\( I_j = \\) { \\(i \| q(x_i) = j \\) } 表示第 \\(j\\) 片叶子上的样本，于是上式可以重新写为

![](https://ws2.sinaimg.cn/large/006tNc79gy1ftgg2jag1kj30q8080758.jpg)

如果固定回归树的结构 \\(q(x)\\) ,每片叶子 \\(j\\) 的最优权重 \\(w_j^{*}\\) 的求解十分简单，结果如下

![](https://ws3.sinaimg.cn/large/006tNc79gy1ftgg41cyoxj30du040jrk.jpg)

由此得到对应的函数值为

![](https://ws3.sinaimg.cn/large/006tNc79gy1ftggcjoi36j30kk04gjru.jpg)

一种直接的方法是穷举所有可能的树结构，然后比较函数值得到最优的回归树。实现方法如下：

从一个单独的叶子开始慢慢划分，对每一片叶子，考虑每一个特征，从左到右依次排列，每一种划分 \\( I = I_L \cup I_R \\\)，计算依此划分后的损失减少量来决定是否要进行划分：

![](https://ws3.sinaimg.cn/large/006tNc79gy1ftgglypf87j30v8040q3h.jpg)



### 2.3 Shrinkage and Column Subsampling

- 
  shrinkage scales newly added weights by a factor \\(\eta\\) after each step of tree boosting (Introduced  by Friedman [stochastic gradient boosting])			
- column subsampling（防止过拟合，加速并行算法的计算）

## 3、分裂算法

### 3.1 exact greedy algorithm

如何找到最佳划分是树学习中十分关键的一个问题，exact greedy algorithm 是指穷举所有特征的所有划分的可能性。这种做法需要将数据按照特征值排序，然后依次累计梯度的统计量。具体算法如下：![](https://ws4.sinaimg.cn/large/006tKfTcgy1ftl0vffvlcj30sc0iuwhm.jpg)

### 3.2 Approximate algorithm 

对每个特征分布找到一些分位点，这样不用穷举所有的可能性，大大减少计算量，并且在某些参数设置下效果与精确搜索相似。不同的提出时机决定了两种不同的算法：

- Global method: 在树建立之前便确定好分位点，在每次树的split中使用相同的点
- Local method：每次树的split中都重新计算分位点

### 3.3 Weighted Quantile Sketch

提出分位点是 Approximate algorithm 中最关键的一步，令 

![](https://ws4.sinaimg.cn/large/006tNc79gy1ftmbdqz7dtj30fu01at8q.jpg)

表示第\\(k\\) 个特征的值以及训练实例对应的二阶梯度值，定义一个函数

![](https://ws1.sinaimg.cn/large/006tNc79gy1ftmbfwukyqj30g604s74n.jpg)

由于 \\(l\\) 是凸的，所以\\(h\\) 不小于0。由此通过以下划分找到划分点 { \\(s_{k1},s_{k2},\cdots, s_{kl}\\) } 使得

![](https://ws4.sinaimg.cn/large/006tNc79gy1ftmblhbnd5j30nm028t8u.jpg)

其中 \\(\epsilon\\) 是近似因子。相当于以样本点的二阶导为权重进行划分。直观的来讲，将会得到大概 \\(\frac{1}{\epsilon}\\) 个划分点。

### 3.4 Sparsity-aware Split Finding

实际问题中的很多输入 x 都是稀疏的，因此提出对每个节点设置一个默认方向，当稀疏矩阵 x 数据缺失时

，样本被划入默认方向。由于每个分支只有两个方向可以选择，我们使用以下算法来确认每个节点处的默认方向。

![](https://ws1.sinaimg.cn/large/006tNc79gy1ftmcin1rfej30pu0tk0xz.jpg)

实验表明 sparsity aware 的算法运行速度大大提升了。

![](https://ws4.sinaimg.cn/large/006tNc79gy1ftmcjuyx7oj30r60j0q5t.jpg)

