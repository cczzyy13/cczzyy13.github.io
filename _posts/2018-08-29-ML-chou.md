---
title: "Machine-Learning-zhzhou"
categories:
- 机器学习
tags:
- 学习笔记
toc: true
---

{% include lib/mathjax.html %}



## 第 1 章 模型选择与评估

### 归纳偏好

任何一个有效的机器学习算法必有归纳偏好。

- 奥卡姆剃刀：

若有多个假设与观察一致，则选最简单的那个。

- NFL定理（没有免费的午餐, No Free Lunch Theorem）：

总误差与学习算法无关。任意两个算法的性能期望相同。

前提是：所有 ‘问题’ 出现的机会相同，或所有问题同等重要。

寓意：脱离具体的问题，空泛的谈论“什么学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。要谈论算法的优劣，必须要针对具体的学习问题。

### 训练

通常把学得模型在实际使用中遇到的数据称为测试数据。

模型评估与选择中用于评估测试的数据集称为验证集。

例如：在研究不同算法的泛化性能时，我们用**测试集**上的判别效果来估计模型在实际使用时的泛化能力。而把数据另外划分为训练集和验证集，基于**验证集**上的表现来进行模型选择和调参。



### 性能评估

对于二分类问题：![](https://ws2.sinaimg.cn/large/006tNbRwgy1fuiaexdpe9j30ci06edgu.jpg)



查准率：

$$Precision = \frac{TP}{TP + FP}$$

查全率：

$$Recall = \frac{TP}{TP + FN} $$

P-R 曲线：根据学习器的预测结构对样例进行排序，前面的是学习器认为最可能是正例的样本，按顺序逐个将样本预测为正例，并计算当前的查全率和查准率，绘制P-R曲线如下：

![](https://ws4.sinaimg.cn/large/006tNbRwgy1fuiap1ac3ij30nu0ci0wf.jpg)

平衡点 BEP (Break-Event Point):

查准率等于查全率时的取值

F1: 查准率与查全率的调和平均

$$F1 = \frac{2\times P\times R }{P + R}$$

ROC 曲线：根据学习器的结构对样本进行排序，并把样本按顺序逐个预测为正例，计算 TPR 和

纵轴 “真正例率” TPR（ True Positive Rate）：

$$TPR = \frac{TP}{TP + FN}$$

横轴 “假正例率” FPR （False Positive Rate）：

$$FPR = \frac{FP}{FP + TN}$$

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuiezt0ejpj30n40caaga.jpg)

AUC(Area Under ROC Curve):ROC 曲线下的面积。

“偏差-方差分解”：泛化误差可分解为偏差、方差与噪声之和。

- 偏差：度量了学习算法的期望预测与真实结果的偏离度，即刻画了学习算法本身的拟合能力。
- 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所导致的影响。

- 噪声：表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

![](https://ws3.sinaimg.cn/large/006tNbRwgy1fuiftj8fkmj30n80l4wl2.jpg)





## 第 5 章  神经网络

### 5.1 定义

**神经网络** 是由具有适用性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应。机器学习中所讨论的神经网络实际上指的是“神经网络学习”（机器学习与神经网络两个学科领域的交叉部分）。

**神经元模型** 在 M-P 神经元模型中，神经元接受到来自 n 个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将和神经元的阈值进行比较，通过“激活函数”处理以产生神经元的输出。



![](https://ws3.sinaimg.cn/large/006tNbRwly1fxorqx2fxpj30og0fmtcs.jpg)

**激活函数** 典型的激活函数有阶跃函数、Sigmoid 函数、径向基函数等。

![](https://ws1.sinaimg.cn/large/006tNbRwly1fxorva5f19j30so0g4774.jpg)

### 5.2 感知机

感知机（Perceptron）由两层神经元组成，可以容易地实现逻辑与、或、非运算。

![](https://ws2.sinaimg.cn/large/006tNbRwly1fxovk4ctqcj30jg0ay75g.jpg)

感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，学习能力十分有限。事实上，与、或、非问题都是线性可分问题，即存在一个线性超平面能将它们分开，则感知机的学习过程一定会收敛，否则感知机学习过程将会发生振荡。



![](https://ws3.sinaimg.cn/large/006tNbRwly1fxovs9joc1j30ps0lodmm.jpg)

### 5.3 多层前馈神经网络

在输出层与输入层之间的神经元层被称为隐层或隐含层，隐含层和输出层神经元都是拥有激活函数的功能神经元。每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构通常称为“**多层前馈神经网络**”。

![](https://ws1.sinaimg.cn/large/006tNbRwly1fxovyqfg4vj30gu0c0juy.jpg)

### 5.4 误差逆传播算法

误差逆传播算法（Error BackPropagation，简称BP）:

![](https://ws3.sinaimg.cn/large/006tNbRwly1fxow55pvufj30o40emwkv.jpg)

假设隐层和输出层神经元都使用 Sigmoid 函数。

对训练例 \\( (x_k, y_k )\\)  ，假定神经网络的输出为 \\( \hat{y}_k = (\hat{y}_1^k, \hat{y}_2^k, \cdots, \hat{y}_l^k)\\)，即

$$\hat{y}_j^k = f(\beta_j - \theta_j) , $$

则网络在\\(x_k, y_k\\) 上的均方误差为

$$E_k = \frac{1}{2} \sum_{j=1}^l (\hat{y}_j^k - y_j^k)^2.$$

网络中一共有 \\((d + l + 1)q + l\\) 个参数需确定，BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，对误差 \\(E_k\\)，给定学习率 \\(\eta\\)，有

$$\Delta w_{hj} = - \eta \frac{\partial E_k}{\partial w_{hj}}.$$

于是

$$\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}_j^k}  \frac{\partial \hat{y}_j^k}{\partial \beta_j}  \frac{\partial \beta_j}{\partial w_{hj}}. $$

Sigmoid 函数有一个很好的性质：\\(f^{'}(x) = f(x)(1 - f(x)) \\)，

因此，令 

$$g_j = -\frac{\partial E_k}{\partial \hat{y}_j^k}  \frac{\partial \hat{y}_j^k}{\partial \beta_j} = -(\hat{y}_j^k - y_j^k)f^{'}(\beta_j - \theta_j) = \hat{y}_j^k(1-\hat{y}_j^k)(y_j^k - \hat{y}_j^k). $$

由此得到BP算法中关于\\(w_{hj}\\) 的更新公式：

$$\Delta w_{hj} = \eta g_j b_h.$$

类似可得：

$$\Delta \theta_j = -\eta g_j,$$

$$\Delta v_{ih} = \eta e_h x_i,$$

$$\Delta \gamma_h = -\eta e_h.$$

学习率 \\( \eta \in (0,1)\\) 控制算法每一轮迭代中的更新步长。

![](https://ws4.sinaimg.cn/large/006tNbRwly1fxp1ab677ij30p80dogq3.jpg)

BP算法的目标是要最小化训练集 \\(D\\)上的累积误差，上面介绍的“**标准BP算法**” 每次仅针对一个训练样例更新连接权和阈值，如果类似地推导出基于累积误差最小化的更新规则，就得到了累积误差逆传播算法。



### 5.5 防止过拟合的策略

BP神经网络经常遭遇过拟合，其训练误差持续性降低，但测试误差却可能上升，可采取的策略是：

1. 早停：将数据分成训练集和验证集，训练集用于计算梯度、更新连接权和阈值，验证集用于估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。

2. 正则化：在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阈值的平方和，则误差目标函数如下：

   $$E = \lambda \frac{1}{m}\sum_{k=1}^m E_k + (1-\lambda)\sum_i w_i^2.$$








