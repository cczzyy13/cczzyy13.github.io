---
title: "机器学习"
categories:
- 机器学习
tags:
- 学习笔记
toc: true
---

{% include lib/mathjax.html %}

## 第 5 章  神经网络

### 5.1 定义

**神经网络** 是由具有适用性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应。机器学习中所讨论的神经网络实际上指的是“神经网络学习”（机器学习与神经网络两个学科领域的交叉部分）。

**神经元模型** 在 M-P 神经元模型中，神经元接受到来自 n 个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将和神经元的阈值进行比较，通过“激活函数”处理以产生神经元的输出。



![](https://ws3.sinaimg.cn/large/006tNbRwly1fxorqx2fxpj30og0fmtcs.jpg)

**激活函数** 典型的激活函数有阶跃函数、Sigmoid 函数、径向基函数等。

![](https://ws1.sinaimg.cn/large/006tNbRwly1fxorva5f19j30so0g4774.jpg)

### 5.2 感知机

感知机（Perceptron）由两层神经元组成，可以容易地实现逻辑与、或、非运算。

![](https://ws2.sinaimg.cn/large/006tNbRwly1fxovk4ctqcj30jg0ay75g.jpg)

感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，学习能力十分有限。事实上，与、或、非问题都是线性可分问题，即存在一个线性超平面能将它们分开，则感知机的学习过程一定会收敛，否则感知机学习过程将会发生振荡。



![](https://ws3.sinaimg.cn/large/006tNbRwly1fxovs9joc1j30ps0lodmm.jpg)

### 5.3 多层前馈神经网络

在输出层与输入层之间的神经元层被称为隐层或隐含层，隐含层和输出层神经元都是拥有激活函数的功能神经元。每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接，这样的神经网络结构通常称为“**多层前馈神经网络**”。

![](https://ws1.sinaimg.cn/large/006tNbRwly1fxovyqfg4vj30gu0c0juy.jpg)

### 5.4 误差逆传播算法

误差逆传播算法（Error BackPropagation，简称BP）:

![](https://ws3.sinaimg.cn/large/006tNbRwly1fxow55pvufj30o40emwkv.jpg)

假设隐层和输出层神经元都使用 Sigmoid 函数。

对训练例 \\( (x_k, y_k )\\)  ，假定神经网络的输出为 \\( \hat{y}_k = (\hat{y}_1^k, \hat{y}_2^k, \cdots, \hat{y}_l^k)\\)，即

$$\hat{y}_j^k = f(\beta_j - \theta_j) , $$

则网络在\\(x_k, y_k\\) 上的均方误差为

$$E_k = \frac{1}{2} \sum_{j=1}^l (\hat{y}_j^k - y_j^k)^2.$$

网络中一共有 \\((d + l + 1)q + l\\) 个参数需确定，BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整，对误差 \\(E_k\\)，给定学习率 \\(\eta\\)，有

$$\Delta w_{hj} = - \eta \frac{\partial E_k}{\partial w_{hj}}.$$

于是

$$\frac{\partial E_k}{\partial w_{hj}} = \frac{\partial E_k}{\partial \hat{y}_j^k}  \frac{\partial \hat{y}_j^k}{\partial \beta_j}  \frac{\partial \beta_j}{\partial w_{hj}}. $$

Sigmoid 函数有一个很好的性质：\\(f^{'}(x) = f(x)(1 - f(x)) \\)，

因此，令 

$$g_j = -\frac{\partial E_k}{\partial \hat{y}_j^k}  \frac{\partial \hat{y}_j^k}{\partial \beta_j} = -(\hat{y}_j^k - y_j^k)f^{'}(\beta_j - \theta_j) = \hat{y}_j^k(1-\hat{y}_j^k)(y_j^k - \hat{y}_j^k). $$

由此得到BP算法中关于\\(w_{hj}\\) 的更新公式：

$$\Delta w_{hj} = \eta g_j b_h.$$

类似可得：

$$\Delta \theta_j = -\eta g_j,$$

$$\Delta v_{ih} = \eta e_h x_i,$$

$$\Delta \gamma_h = -\eta e_h.$$

学习率 \\( \eta \in (0,1)\\) 控制算法每一轮迭代中的更新步长。

![](https://ws4.sinaimg.cn/large/006tNbRwly1fxp1ab677ij30p80dogq3.jpg)

BP算法的目标是要最小化训练集 \\(D\\)上的累积误差，上面介绍的“**标准BP算法**” 每次仅针对一个训练样例更新连接权和阈值，如果类似地推导出基于累积误差最小化的更新规则，就得到了累积误差逆传播算法。



### 5.5 防止过拟合的策略

BP神经网络经常遭遇过拟合，其训练误差持续性降低，但测试误差却可能上升，可采取的策略是：

1. 早停：将数据分成训练集和验证集，训练集用于计算梯度、更新连接权和阈值，验证集用于估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。

2. 正则化：在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阈值的平方和，则误差目标函数如下：

   $$E = \lambda \frac{1}{m}\sum_{k=1}^m E_k + (1-\lambda)\sum_i w_i^2.$$








